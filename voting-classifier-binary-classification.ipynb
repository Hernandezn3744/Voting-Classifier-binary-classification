{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\nx = load_breast_cancer(as_frame=True).data\ny = load_breast_cancer(as_frame=True).target\n\ndf = x.copy()\ndf['target'] = y.copy()\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:36.161990Z","iopub.execute_input":"2023-06-09T03:33:36.162344Z","iopub.status.idle":"2023-06-09T03:33:37.543180Z","shell.execute_reply.started":"2023-06-09T03:33:36.162322Z","shell.execute_reply":"2023-06-09T03:33:37.541936Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                   0.07871  ...          17.33           184.60      2019.0   \n1                   0.05667  ...          23.41           158.80      1956.0   \n2                   0.05999  ...          25.53           152.50      1709.0   \n3                   0.09744  ...          26.50            98.87       567.7   \n4                   0.05883  ...          16.67           152.20      1575.0   \n..                      ...  ...            ...              ...         ...   \n564                 0.05623  ...          26.40           166.10      2027.0   \n565                 0.05533  ...          38.25           155.00      1731.0   \n566                 0.05648  ...          34.12           126.70      1124.0   \n567                 0.07016  ...          39.42           184.60      1821.0   \n568                 0.05884  ...          30.37            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  target  \n0                  0.2654          0.4601                  0.11890       0  \n1                  0.1860          0.2750                  0.08902       0  \n2                  0.2430          0.3613                  0.08758       0  \n3                  0.2575          0.6638                  0.17300       0  \n4                  0.1625          0.2364                  0.07678       0  \n..                    ...             ...                      ...     ...  \n564                0.2216          0.2060                  0.07115       0  \n565                0.1628          0.2572                  0.06637       0  \n566                0.1418          0.2218                  0.07820       0  \n567                0.2650          0.4087                  0.12400       0  \n568                0.0000          0.2871                  0.07039       1  \n\n[569 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:37.546295Z","iopub.execute_input":"2023-06-09T03:33:37.547001Z","iopub.status.idle":"2023-06-09T03:33:37.561290Z","shell.execute_reply.started":"2023-06-09T03:33:37.546948Z","shell.execute_reply":"2023-06-09T03:33:37.559965Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"mean radius                0\nmean texture               0\nmean perimeter             0\nmean area                  0\nmean smoothness            0\nmean compactness           0\nmean concavity             0\nmean concave points        0\nmean symmetry              0\nmean fractal dimension     0\nradius error               0\ntexture error              0\nperimeter error            0\narea error                 0\nsmoothness error           0\ncompactness error          0\nconcavity error            0\nconcave points error       0\nsymmetry error             0\nfractal dimension error    0\nworst radius               0\nworst texture              0\nworst perimeter            0\nworst area                 0\nworst smoothness           0\nworst compactness          0\nworst concavity            0\nworst concave points       0\nworst symmetry             0\nworst fractal dimension    0\ntarget                     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# scale the data so...\n# we don't get yelled at by a computer\nx_scaled = MinMaxScaler().fit_transform(x) ","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:37.562634Z","iopub.execute_input":"2023-06-09T03:33:37.562987Z","iopub.status.idle":"2023-06-09T03:33:37.574844Z","shell.execute_reply.started":"2023-06-09T03:33:37.562958Z","shell.execute_reply":"2023-06-09T03:33:37.573747Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# binary\nfrom sklearn.linear_model import (LogisticRegression, RidgeClassifier, Perceptron, SGDClassifier, \nPassiveAggressiveClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier, \nGradientBoostingClassifier)\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\n\n# binary classifiers\nlog_regression = LogisticRegression(random_state=21)\nridgeclass = RidgeClassifier(random_state=21)\nknn = KNeighborsClassifier()\ndecision_tree = DecisionTreeClassifier(random_state=21)\nrandom_forest = RandomForestClassifier(random_state=21)\nextra_trees = ExtraTreesClassifier(random_state=21)\nada_classifier = AdaBoostClassifier(random_state=21)\ngradient_booster = GradientBoostingClassifier(random_state=21)\nsvc = SVC(random_state=21)\nlda = LinearDiscriminantAnalysis()\nperceptron = Perceptron(random_state=21)\nmlp_classifier = MLPClassifier(random_state=21, solver='lbfgs')\n\n# normally distributed\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# norm classifiers\ngauss_nb = GaussianNB()\nmulti_np = MultinomialNB()\nbernoulli_nb = BernoulliNB()\nqda = QuadraticDiscriminantAnalysis()\n\n# online classifiers\nsgd = SGDClassifier(random_state=21)\npassive_aggressive = PassiveAggressiveClassifier(random_state=21)\n\nlinear_svc = LinearSVC()\nsgdc = SGDClassifier()\ncomp_nb = ComplementNB()\n\nall_classifiers = {\n    'Logistic Regression': log_regression,\n    'Ridge Classifier': ridgeclass,\n    'K Nearest Neighbors (KNN)': knn,\n    'Decision Tree': decision_tree,\n    'Random Forest': random_forest,\n    'Extra Trees Classifier': extra_trees,\n    'AdaBoost': ada_classifier,\n    'Gradient Boosting Classifier': gradient_booster,\n    'Support Vector Machines (SVM)': svc,\n    'Linear Discriminant Analysis (LDA)': lda,\n    'Perceptron': perceptron,\n    'Neural Network (MLPClassifier)': mlp_classifier,\n    'Gaussian Naive Bayes': gauss_nb,\n    'Multinomial Naive Bayes': multi_np,\n    'Complement Naive Bayes': comp_nb,\n    'Bernoulli Naive Bayes': bernoulli_nb,\n    'Quadratic Discriminant Analysis (QDA)': qda,\n    'Stochastic Gradient Descent (SGD) Classifier': sgd,\n    'Passive Aggressive Classifier': passive_aggressive,   \n    'Linear Support Vector Classifier (LinearSVC)': linear_svc,\n    'Stochastic Gradient Descent Classifier (SGDClassifier)': sgdc\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:37.576923Z","iopub.execute_input":"2023-06-09T03:33:37.577503Z","iopub.status.idle":"2023-06-09T03:33:38.075069Z","shell.execute_reply.started":"2023-06-09T03:33:37.577473Z","shell.execute_reply":"2023-06-09T03:33:38.074070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# split\nfrom sklearn.model_selection import train_test_split\n# metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n# score function\ndef accuracy(pred, y_test):\n    return accuracy_score(pred,y_test)\n# split data function\ndef split(data, target):\n    xtrain, xtest, ytrain, ytest = train_test_split(\n        data, target, test_size=0.2, random_state=21, stratify=target)\n    return xtrain, xtest, ytrain, ytest\n# train function\ndef train(model, xtrain, ytrain):\n    model.fit(xtrain,ytrain)\n    val = model.score(xtrain,ytrain)\n    return model, val\n# test function\ndef test(model, xtest):\n    pred = model.predict(xtest)\n    return pred\n# together now\ndef together(data, target, model):\n    # split\n    x_train, x_test, y_train, y_test = split(data,target)\n    # train\n    fitted_model, train_score = train(model,x_train,y_train)\n    # test\n    pred = test(fitted_model,x_test)\n    # score\n    test_score = accuracy(pred,y_test)\n    return  fitted_model, round(train_score,4), round(test_score,4), pred, y_test\n# sum false values\ndef false_classified_score(matrix):\n    byn = []\n    for i in range(len(matrix)):\n        for j in range(len(matrix)):\n            if i != j:\n                byn.append(matrix[i,j])\n    return sum(byn)\n\n# confusion score\ndef performance_matrix(ytest, pred):\n    cm = confusion_matrix(ytest,pred)\n    return cm\n# all together now!\ndef all_together(model_dict, data, target):\n    results = []\n    for i, key in enumerate(model_dict):\n        model, train_score, test_score, pred, ytest = together(data, target, model_dict[key])\n        matrix = performance_matrix(ytest,pred)\n        false_sum = false_classified_score(matrix)\n        results.append((model, # model, 0\n                        train_score, test_score, # train/test accuracy, 1, 2\n                        round(abs(train_score-test_score),4), # difference between train/test, 3\n                        round(precision_score(ytest, pred),4), # percision, 4\n                        round(recall_score(ytest, pred),4), # recall, 5\n                        round(f1_score(ytest, pred),4), # f1, 6\n                        false_sum # sum of false classified, 7\n                       ))\n    #for result in results:\n    #    print('model: ', result[0])\n    #    print('train score: ', result[1])\n    #    print('test score: ', result[2])\n    #    print('train/test diff: ', result[3])\n    #    print('percision: ', result[4])\n    #    print('recall: ', result[5])\n    #    print('f1: ', result[6])\n    #    print('misclassified total: ', result[7])\n    #    print('-'*40)\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:38.076286Z","iopub.execute_input":"2023-06-09T03:33:38.077177Z","iopub.status.idle":"2023-06-09T03:33:38.091228Z","shell.execute_reply.started":"2023-06-09T03:33:38.077125Z","shell.execute_reply":"2023-06-09T03:33:38.089549Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# now save the result to a data frame...\nres = all_together(all_classifiers, x_scaled, y)\nmodel_df = pd.DataFrame(res, columns=['model', 'train_score', 'test_score','train_test diff',\n                                     'percision','recall','f1','misclassified total'])\nmodel_df","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:38.092765Z","iopub.execute_input":"2023-06-09T03:33:38.093124Z","iopub.status.idle":"2023-06-09T03:33:39.889457Z","shell.execute_reply.started":"2023-06-09T03:33:38.093096Z","shell.execute_reply":"2023-06-09T03:33:39.888572Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                model  train_score  \\\n0                 LogisticRegression(random_state=21)       0.9670   \n1                    RidgeClassifier(random_state=21)       0.9626   \n2                              KNeighborsClassifier()       0.9758   \n3             DecisionTreeClassifier(random_state=21)       1.0000   \n4   (DecisionTreeClassifier(max_features='sqrt', r...       1.0000   \n5   (ExtraTreeClassifier(random_state=209271753), ...       1.0000   \n6   (DecisionTreeClassifier(max_depth=1, random_st...       1.0000   \n7   ([DecisionTreeRegressor(criterion='friedman_ms...       1.0000   \n8                                SVC(random_state=21)       0.9824   \n9                        LinearDiscriminantAnalysis()       0.9604   \n10                        Perceptron(random_state=21)       0.8989   \n11     MLPClassifier(random_state=21, solver='lbfgs')       1.0000   \n12                                       GaussianNB()       0.9429   \n13                                    MultinomialNB()       0.8396   \n14                                     ComplementNB()       0.8615   \n15                                      BernoulliNB()       0.6308   \n16                    QuadraticDiscriminantAnalysis()       0.9714   \n17                     SGDClassifier(random_state=21)       0.9802   \n18       PassiveAggressiveClassifier(random_state=21)       0.9473   \n19                                        LinearSVC()       0.9780   \n20                                    SGDClassifier()       0.9758   \n\n    test_score  train_test diff  percision  recall      f1  \\\n0       0.9737           0.0067     0.9600  1.0000  0.9796   \n1       0.9649           0.0023     0.9474  1.0000  0.9730   \n2       0.9912           0.0154     0.9863  1.0000  0.9931   \n3       0.9561           0.0439     0.9718  0.9583  0.9650   \n4       0.9561           0.0439     0.9589  0.9722  0.9655   \n5       0.9737           0.0263     0.9859  0.9722  0.9790   \n6       0.9649           0.0351     0.9722  0.9722  0.9722   \n7       0.9474           0.0526     0.9342  0.9861  0.9595   \n8       1.0000           0.0176     1.0000  1.0000  1.0000   \n9       0.9649           0.0045     0.9474  1.0000  0.9730   \n10      0.8772           0.0217     0.8372  1.0000  0.9114   \n11      0.9649           0.0351     0.9857  0.9583  0.9718   \n12      0.9298           0.0131     0.9571  0.9306  0.9437   \n13      0.8246           0.0150     0.7826  1.0000  0.8780   \n14      0.8684           0.0069     0.8800  0.9167  0.8980   \n15      0.6140           0.0168     0.6250  0.9722  0.7609   \n16      0.9649           0.0065     0.9595  0.9861  0.9726   \n17      0.9825           0.0023     0.9861  0.9861  0.9861   \n18      0.9298           0.0175     0.9000  1.0000  0.9474   \n19      0.9912           0.0132     0.9863  1.0000  0.9931   \n20      0.9825           0.0067     0.9730  1.0000  0.9863   \n\n    misclassified total  \n0                     3  \n1                     4  \n2                     1  \n3                     5  \n4                     5  \n5                     3  \n6                     4  \n7                     6  \n8                     0  \n9                     4  \n10                   14  \n11                    4  \n12                    8  \n13                   20  \n14                   15  \n15                   44  \n16                    4  \n17                    2  \n18                    8  \n19                    1  \n20                    2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>train_score</th>\n      <th>test_score</th>\n      <th>train_test diff</th>\n      <th>percision</th>\n      <th>recall</th>\n      <th>f1</th>\n      <th>misclassified total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LogisticRegression(random_state=21)</td>\n      <td>0.9670</td>\n      <td>0.9737</td>\n      <td>0.0067</td>\n      <td>0.9600</td>\n      <td>1.0000</td>\n      <td>0.9796</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RidgeClassifier(random_state=21)</td>\n      <td>0.9626</td>\n      <td>0.9649</td>\n      <td>0.0023</td>\n      <td>0.9474</td>\n      <td>1.0000</td>\n      <td>0.9730</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>KNeighborsClassifier()</td>\n      <td>0.9758</td>\n      <td>0.9912</td>\n      <td>0.0154</td>\n      <td>0.9863</td>\n      <td>1.0000</td>\n      <td>0.9931</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier(random_state=21)</td>\n      <td>1.0000</td>\n      <td>0.9561</td>\n      <td>0.0439</td>\n      <td>0.9718</td>\n      <td>0.9583</td>\n      <td>0.9650</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n      <td>1.0000</td>\n      <td>0.9561</td>\n      <td>0.0439</td>\n      <td>0.9589</td>\n      <td>0.9722</td>\n      <td>0.9655</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(ExtraTreeClassifier(random_state=209271753), ...</td>\n      <td>1.0000</td>\n      <td>0.9737</td>\n      <td>0.0263</td>\n      <td>0.9859</td>\n      <td>0.9722</td>\n      <td>0.9790</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n      <td>1.0000</td>\n      <td>0.9649</td>\n      <td>0.0351</td>\n      <td>0.9722</td>\n      <td>0.9722</td>\n      <td>0.9722</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n      <td>1.0000</td>\n      <td>0.9474</td>\n      <td>0.0526</td>\n      <td>0.9342</td>\n      <td>0.9861</td>\n      <td>0.9595</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SVC(random_state=21)</td>\n      <td>0.9824</td>\n      <td>1.0000</td>\n      <td>0.0176</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LinearDiscriminantAnalysis()</td>\n      <td>0.9604</td>\n      <td>0.9649</td>\n      <td>0.0045</td>\n      <td>0.9474</td>\n      <td>1.0000</td>\n      <td>0.9730</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Perceptron(random_state=21)</td>\n      <td>0.8989</td>\n      <td>0.8772</td>\n      <td>0.0217</td>\n      <td>0.8372</td>\n      <td>1.0000</td>\n      <td>0.9114</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>MLPClassifier(random_state=21, solver='lbfgs')</td>\n      <td>1.0000</td>\n      <td>0.9649</td>\n      <td>0.0351</td>\n      <td>0.9857</td>\n      <td>0.9583</td>\n      <td>0.9718</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>GaussianNB()</td>\n      <td>0.9429</td>\n      <td>0.9298</td>\n      <td>0.0131</td>\n      <td>0.9571</td>\n      <td>0.9306</td>\n      <td>0.9437</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>MultinomialNB()</td>\n      <td>0.8396</td>\n      <td>0.8246</td>\n      <td>0.0150</td>\n      <td>0.7826</td>\n      <td>1.0000</td>\n      <td>0.8780</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ComplementNB()</td>\n      <td>0.8615</td>\n      <td>0.8684</td>\n      <td>0.0069</td>\n      <td>0.8800</td>\n      <td>0.9167</td>\n      <td>0.8980</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>BernoulliNB()</td>\n      <td>0.6308</td>\n      <td>0.6140</td>\n      <td>0.0168</td>\n      <td>0.6250</td>\n      <td>0.9722</td>\n      <td>0.7609</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>QuadraticDiscriminantAnalysis()</td>\n      <td>0.9714</td>\n      <td>0.9649</td>\n      <td>0.0065</td>\n      <td>0.9595</td>\n      <td>0.9861</td>\n      <td>0.9726</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>SGDClassifier(random_state=21)</td>\n      <td>0.9802</td>\n      <td>0.9825</td>\n      <td>0.0023</td>\n      <td>0.9861</td>\n      <td>0.9861</td>\n      <td>0.9861</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>PassiveAggressiveClassifier(random_state=21)</td>\n      <td>0.9473</td>\n      <td>0.9298</td>\n      <td>0.0175</td>\n      <td>0.9000</td>\n      <td>1.0000</td>\n      <td>0.9474</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>LinearSVC()</td>\n      <td>0.9780</td>\n      <td>0.9912</td>\n      <td>0.0132</td>\n      <td>0.9863</td>\n      <td>1.0000</td>\n      <td>0.9931</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>SGDClassifier()</td>\n      <td>0.9758</td>\n      <td>0.9825</td>\n      <td>0.0067</td>\n      <td>0.9730</td>\n      <td>1.0000</td>\n      <td>0.9863</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create votining classifier...\nel_publico = VotingClassifier(\n    estimators=list(all_classifiers.items()),\n    voting='hard')\n\nballot_res = all_together({'voting_clf': el_publico},x_scaled,y)\n\n# copy paste...\nprint('¡¿Qué dice el público?!')\npd.DataFrame(ballot_res, columns=['model', 'train_score', 'test_score','train_test diff',\n                                     'percision','recall','f1','misclassified total'])","metadata":{"execution":{"iopub.status.busy":"2023-06-09T03:33:39.890532Z","iopub.execute_input":"2023-06-09T03:33:39.891014Z","iopub.status.idle":"2023-06-09T03:33:41.394321Z","shell.execute_reply.started":"2023-06-09T03:33:39.890985Z","shell.execute_reply":"2023-06-09T03:33:41.393660Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"¡¿Qué dice el público?!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                               model  train_score  test_score  \\\n0  VotingClassifier(estimators=[('Logistic Regres...       0.9846      0.9825   \n\n   train_test diff  percision  recall      f1  misclassified total  \n0           0.0021      0.973     1.0  0.9863                    2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>train_score</th>\n      <th>test_score</th>\n      <th>train_test diff</th>\n      <th>percision</th>\n      <th>recall</th>\n      <th>f1</th>\n      <th>misclassified total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VotingClassifier(estimators=[('Logistic Regres...</td>\n      <td>0.9846</td>\n      <td>0.9825</td>\n      <td>0.0021</td>\n      <td>0.973</td>\n      <td>1.0</td>\n      <td>0.9863</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}